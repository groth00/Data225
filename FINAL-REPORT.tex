
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[11pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
%\ifCLASSINFOpdf
\usepackage{graphicx}
  % declare the path(s) where your graphic files are
%\graphicspath{{../pdf/}{../jpeg/}{../png/}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
%\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{float}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}
\urlstyle{same}
\pagecolor{white}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Analysis of the Yelp Academic Dataset}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Gabriel~Roth,~Tiffany~Yang,~Lianglei~Zhang,~Nataliya~Sobol,~Yixin~Huang,~and~Jie~Dong}% <-this % stops a space
%\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell was with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
%E-mail: see http://www.michaelshell.org/contact.html
%\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
The ability of users to share their experiences and personal opinions about businesses online has greatly impacted how businesses operate today. The presence of review websites such as Yelp and even social media websites such as Google, Facebook, and Instagram provide a constant amount of data for businesses to analyze. However, this data is not easily numerically summarized and is semi-structured or unstructured. In this paper, we use SQL queries and Natural Language Processing to analyze businesses in the Yelp Open Dataset. Using MySQL and BigQuery, we discover historical trends from business metadata and behavioral patterns from user metadata. Using Natural Language Processing in Spark NLP, businesses are evaluated based on user sentiments in text reviews. These insights will allow businesses to evaluate themselves, compare themselves to competitors, and improve their services. \\
\\
Keywords: Yelp, Sentiment Analysis, Spark NLP, SQL, BigQuery, Data Studio
\end{abstract}
}
% Note that keywords are not normally used for peerreview papers.
%\begin{IEEEkeywords}
%Computer Society, IEEE, IEEEtran, journal, \LaTeX, paper, template.
%\end{IEEEkeywords}}

% make the title area
\maketitle
% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.

\IEEEraisesectionheading{\section{Introduction}\label{sec:Introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.

% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
\IEEEPARstart{T}{he} existence of websites that allow users to review businesses is important to both users and businesses. Users want information that can help them choose between places to visit or restaurants to eat at. Businesses seek feedback to understand what customers like and dislike and use it to improve the quality of their services. Yelp is one of many websites that collect user reviews and serve as an advertising platform for businesses to attract users. Although Yelp contains reviews for thousands of businesses, what users and businesses will prioritize is the average star rating. Star rating is a single number, making it easy and quick for people to use. If a customer was deciding between two restaurants having a three-star rating and a four-star rating, it is obvious to pick the latter. Consequently, it is difficult to give detailed and fair evaluations of businesses without manually reading through several reviews and using personal judgment to determine whether the reviewer is trustworthy or not. \\
\indent The Yelp academic dataset provides data about businesses, users, and reviews. By analyzing business and user metadata, information about historical trends can be used for businesses to compare themselves to other nearby businesses. User metadata can be used to understand user behavior and the relationship between users and reviews, allowing businesses to detect rating patterns. Natural Language Processing helps process the text reviews by extracting important parts and determining user sentiment. Using Python, MySQL, Spark NLP, and Google BigQuery, we process the data, persist it into databases, and query it to generate multiple insights. From the results in BigQuery, visualizations are generated using Data Studio to create easily interpretable graphs for the identification of patterns. \\
\indent The rest of the paper is as follows: a section on previous work done on the Yelp dataset and concepts for analyzing semi-structured data, a section on data modeling, a section on methodology and tools used to complete our work, a section on results and visualizations, a section on lessons learned, and finally the conclusion.
% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol
\section{Literature Review}
Previous research on the Yelp dataset involved analysis of text reviews to provide business-specific feedback. Ching and Bulos (2019) performed sentiment analysis and opinion mining using the AYLIEN Text Analysis API, time series forecasting with linear regression, and recommended business strategies based on predicted data [1]. The motivation behind the paper was to help businesses adapt to customers' changing needs and demands by analyzing the growing amount of information from social media and review sites. In particular, mining the text data from Yelp provided information on each business’s strong and weak points. \\
\indent The paper only considered five fast-food chains: Burger King, KFC, McDonald’s, Popeye's, and Subway. In addition, only the first thousand entries for each restaurant were analyzed.
The AYLIEN API provided a model that took a string input, mined it, and returned aspects and polarity. The researchers conducted a survey of five people per restaurant, twenty-five people total, in order to create a baseline for comparing these aspects and their polarity values. Linear regression was used to generate time-series data; based on this predicted data, the authors provided business-specific recommendations for each aspect. \\
\indent Dealing with semi-structured data is a problem that is frequently encountered; incorporating it into a data warehouse requires a specialized process. While OLAP cubes enable an aggregation-centric analysis of transactional data based on measurable facts with dimensional characteristics, this is not possible for semi-structured data. For semi-structured or unstructured data, deriving a multi-dimensional schema is difficult because the data itself is schema-less. To alleviate this issue, OLAP dimensions can be discovered through a data enrichment layer that involves data mining to detect new elements. These elements can be measures, dimensions, or hierarchy levels and can represent static or dynamic properties of the data. The general process of data enrichment involves identifying parts of the dataset that can be transformed to facts and dimensions, enriching the outcome by using external services, and extending the obtained structures via the content-driven discovery of additional characteristics [2, pp.2]. \\
\indent In our paper, we use BigQuery to perform queries on a larger scale and use Spark NLP to mine restaurant text reviews. In addition to using Pandas in Python to prepare the dataset, we use Cloud Data Fusion, a powerful and fully-managed ETL tool from Google Cloud (GCP) for further data cleansing [3]. The work related to our project can be subdivided into three major sections: (1) integrating semi-structured data into the data warehouse, (2) sentiment analysis, and (3) data analysis and visualization.

\begin{figure*}[h]
\centering
\subfloat{\includegraphics[width=5in]{data-model.png}
\label{fig_first_case}}
\caption{Snowflake schema model}
\label{fig_sim}
\end{figure*}

\section{Data Modeling}
A snowflake schema is used to model the data; the fact table is called Reviews and the dimension tables are called Business, Date, and Users. The tables Categories and Annotation are related to the Business table through business\_id. After dropping irrelevant attributes from the source data, the Business table contains business\_id, name, address, city, state, postal code, latitude, longitude, average star rating, number of reviews, and operating status. The Categories table is used to further specify the restaurant’s categories; there are general categories such as bars and juiceries and ethnic categories such as Chinese, Indian, Thai, Korean, and so on. The categories table only contains business\_id and categories. There is a 1-to-many relationship between Business and Categories, which means a restaurant can belong to multiple categories. The Business table also has a 1-to-many relationship with the Annotation table, which contains business\_id, annotation, and sentiment. The Date table has date\_id, year, month, and day. The Users table contains user\_id, username, user review count, user creation date, and user ratings such as useful, funny, and cool. The Reviews table includes review\_id, user\_id, business\_id, star rating, date, and user votes for useful, funny, and cool. Since Reviews is the fact table, there are 1-to-many relationships between Users and Reviews, Business and Reviews, and Date and Reviews. 


\section{Methodology}
First, the Yelp dataset was downloaded from Kaggle and preprocessed in Python using Pandas. A relational model was developed before importing the business metadata into MySQL using MySQL connector. After creating tables and inserting values, queries were executed for exploratory data analysis. In order to gain further insight from the text reviews, Natural Language Processing was used. \\
\indent Spark NLP provides state-of-the-art Natural Language Processing models and pipelines from deep learning research [4]. It is fully open-source and is built on top of Apache Spark; therefore, it is able to scale well and supports the usual programming languages Java, Scala, and Python. Spark NLP provides many pre-trained pipelines to perform tasks such as text classification, sentiment analysis, and named entity recognition. For example, the Explain Document ML pipeline takes in text input and performs common operations such as tokenization, lemmatization, sentence detection, and parts of speech tagging. Each of the tasks in a pipeline is performed by units called annotators.  \\
\indent The pipeline in this project combined annotators in a custom pipeline. It includes the Document Assembler, Sentence Detector, Tokenizer, Word Embeddings Model, Aspect-Based Sentiment Analysis Named Entity Recognition Model, and Named Entity Converter. The GloVe word embeddings model is a pre-trained DL model that maps words to a space where the distance between words is based on semantic similarity. The NER model is another DL model trained to extract entities from restaurant reviews. The final output of the pipeline are aspects of a review and the user sentiment towards those aspects. The pipeline was applied to a limited number of businesses due to its slowness; the results were concatenated into a single CSV file. This file corresponds to the Annotation table in the data model. \\
\indent The full data was uploaded into a Google Cloud Storage bucket. Using Cloud Data Fusion, an ETL pipeline was designed to extract the data from the bucket, transform each CSV file by renaming columns and converting data types, and load it into BigQuery tables. BigQuery, a cloud-based enterprise data warehouse, was used to run fast SQL queries against the full data and interactively view the results with Data Studio. 

\section{Preliminary Results}
\begin{figure*}[h]
\centering
\subfloat{\includegraphics[width=4in]{map.png}
\label{fig_first_case}}
\caption{Business density}
\label{fig_sim}
\end{figure*}

To explore the business metadata, a relational data model was designed to create the relevant tables and insert the data into MySQL using Python and MySQL Connector. Afterwards, some queries were performed to generate some findings. Figure 2 shows a map indicating the count of businesses by city. For this stage, we decided to analyze Massachusetts businesses in-depth because the city contained the highest amount of businesses in the dataset provided by Kaggle. \\
\indent Many restaurants in Massachusetts belong to more than one category. The restaurant with the most categories is called Sea To You Sushi, which has 20 categories. In addition to selling sushi, it holds cooking classes, food safety training, and event planning services. There are 7 business merchants that belong to over 15 categories. Among them, five of them have restaurants, such as LBC Boutique \& Loan, a luxury boutique that provides appraisals and loans and Harvard Square, which is a comprehensive merchant with Japanese and Korean food and a cultural center for musical performances. \\
\indent The average rating of businesses with more than 50 reviews grouped by category starts at 3.93 stars for Vegan, 3.93 for Seafood Markets, 3.91 for Ramen, 3.91 for Specialty Food, and so on. Again, some of these businesses belong to multiple categories; places can be bars or pubs and belong to general shopping areas. In particular, the city of Cambridge has many juiceries under the vegan category in addition to pizzerias and Indian food. Furthermore, upon looking into the Vegan restaurants, there are only six cities in Massachusetts that have them. The cities are Allston (6), Cambridge (23), Boston (63), Brookline (10), Newton (7), and Somerville (14). The vegan restaurants in Allston have similar latitude and longitude, meaning that they are in the same area. Most vegan restaurants in Cambridge are located on the same street, Massachusetts Avenue. In Brookline, most are located on Harvard Street. These results illustrate how helpful it is to narrow down the analysis to specific restaurants in one city and one category to verify any assumptions about the data.  \\
\indent The top five cities with the most restaurants in Massachusetts are Boston (2846 restaurants), Cambridge (775 restaurants), Somerville (389 restaurants), Quincy (310 restaurants), and Brookline (250 restaurants). The ten cities with the most restaurants in Massachusetts all belong to the Greater Boston area.\\
\indent By changing parameters such as review count and average star rating, some interesting facts are found. In total, Massachusetts has over 10,000 businesses. When the review count is over 20, 6481 merchants’ ratings are above the average rating score of the city. When the city's average star rating is over 4 and the review count is over 50, 34 merchants’ ratings are above the average star rating of the city. When the city's average star rating is over 4.2 and the review count is over 150, only 5 merchants’ ratings are above the average star rating of the city. Therefore, most of the restaurants in Massachusetts have a star rating below four; only a few accomplished restaurants stand out among thousands. \\
\indent Restaurants with a 4 star rating are the most common and in total have 482,400 reviews. Restaurants with a 1 star rating are the least common and have 531 reviews in total. Star rating is useful for distinguishing good restaurants from bad restaurants, but nothing else. Additional analysis requires information from attributes such as review count. 

\section{Demonstration}
The ETL process is performed using Google Cloud Data Fusion. It is a service that allows developers to easily create ETL pipelines using point and click operations to extract data from Cloud Storage, transform it, and sink it into BigQuery. BigQuery, a serverless Data Warehouse that is scalable and can easily process petabytes of data, is used to write SQL queries and generate insights into historical trends. Finally, Data Studio utilizes the results of BigQuery queries to create customizable informative reports and dashboards consisting of bar charts, histograms, maps, and more [5]. \\
\begin{figure*}[!t]
\centering
\subfloat{\includegraphics[width=5in]{fig3.pdf}
\label{fig_first_case}}
\caption{Review count per year segmented by months}
\label{fig_first_case}
\end{figure*}
\begin{figure*}[!t]
\centering
\subfloat{\includegraphics[width=5in]{fig4.pdf}
\label{fig_second_case}}
\caption{User registration and review count for 2019 and 2020}
\label{fig_sim}
\end{figure*}
\indent In this section, businesses will be analyzed based on the review count instead of star rating. Businesses with higher review counts are all American chain stores such as McDonald’s, Chipotle, Panera Bread, Voodoo Doughnut, and The Cheesecake Factory; however, these chain stores often have low star ratings. \\
\indent Figure 3 shows the review count per year separated by month with each three month period colored similarly. The number of Yelp user reviews increased since 2006, peaked in 2018, and declined in the following years. Across all years, the summer months July and August have the highest review counts. Last year, 2020, is an exception, as the majority of businesses closed due to COVID-19. In addition, the declining amount of reviews occurred since 2018, meaning Yelp’s decline of user activity began before the pandemic. One possible explanation according to Whitney is that Yelp faced increasing competition from other platforms like Google, Facebook, Instagram, which contain better photos and provide real-time interaction between users [6]. \\
\begin{figure*}[!t]
\centering
\subfloat{\includegraphics[width=5in]{fig5.pdf}
\label{fig_second_case}}
\caption{User score and usefulness per registration year}
\label{fig_sim}
\end{figure*}
\begin{figure*}[!t]
\centering
\subfloat{\includegraphics[width=5in]{fig6.pdf}
\label{fig_second_case}}
\caption{User and review information per hour}
\label{fig_sim}
\end{figure*}
\indent Figure 4 shows the impact of COVID-19 on Yelp by comparing 2019 data to 2020 data. The points on each line represent the number of new Yelp users per month and the side-by-side bars represent the review count per month in 2019 and 2020. Clearly, the user registration count and review count are both higher in 2019 compared to 2020. In 2020, more people stayed at home and used delivery services such as Uber Eats and Postmates instead of Yelp. In addition, the data changed as the pandemic situation changed. Since the outbreak in February, both registration numbers and review numbers continued to decrease until the end of April. In summer, when daily coronavirus cases started to fall, both numbers gradually rose, though they were still less compared to 2019. Due to the lack of vaccinated people, the cases rose again for the last quarter of 2020, causing less user activity. \\
\indent Next, the user data was analyzed to understand historical trends for user behavior and the relationship between users and reviews. Our assumption is that a user will have more impact when he or she writes more reviews on Yelp. After filtering users with less than ten reviews, the user score is calculated by dividing the user user usefulness by review count; usefulness is determined by other Yelp users (similar to Reddit upvotes). Filtering out users with few reviews is necessary because they could be spam accounts or accounts that are intentionally harming a businesses’ rating by giving low reviews. In addition, these accounts with small sample size will skew the data if they have an overall star rating of 5 or 1. Figure 5 plots the average user score on top of average usefulness over a range of years. The figure illustrates the steady decline of new user score and usefulness each year.  \\
\begin{figure*}[!t]
\centering
\subfloat{\includegraphics[width=3in]{fig7.pdf}
\label{fig_second_case}}
\hfil
\subfloat{\includegraphics[width=4in]{fig9.pdf}
\label{fig_second_case}}
\caption{Proportion of star ratings for users}
\label{fig_sim}
\end{figure*}
\indent Figure 6 shows the total number of reviews submitted by Yelp users during each hour of a day. These reviews can be rated as useful, funny, or cool by other users. The number of useful, funny, and cool reviews are plotted using a stacked vertical bar. Since the proportion of cool votes to funny votes to useful votes are relatively the same for each hour, the usefulness of a review does not change much by hour. Generally, there are more readers than writers because the total number of useful, funny, and cool ratings is higher than the total number of submitted reviews. From 11 PM to 2 AM, more reviews were submitted and less users rated reviews. This makes sense, as most people sleep during that time period. From 6 PM to 2 AM, there are more users rating and writing reviews compared to the other hours. Though 2 AM seems late, it is still reasonable for people living in eastern time zones. To summarize, the figure indicates that the reviews posted during 6 PM to 2 AM will attract more attention. Since more people will read and evaluate reviews during this time period, these reviews are more valuable and can cause users to gain more followers. \\
\indent Figure 7 is about user average star rating; most star ratings for businesses are in the three to four star range, adding up to 60\% of all star ratings. Four and five star ratings account for about 47\% of all star ratings, and three star ratings and lower account for 53\% of all star ratings. As a result, it can be concluded that Yelp reviews are more negative than positive. The adjacent figure is a stacked column chart that displays the distribution of star rating for the most useful users; the leftmost bar is for one star reviews, and the rightmost bar is for five star reviews. With the exception of two users who are neutral, 65-80\% of these users’ ratings are between four and five stars. \\
\indent Using the annotation and sentiment data from Spark NLP, stacked charts were created to display information for a few businesses and their positive and negative sentiment counts. Figure 9 displays the sentiment count over all aspects; these aspects range from specific food dishes to general characteristics, such as ambiance. Figure 10 displays the sentiment counts for individual aspects service and atmosphere. Overall, restaurants were nearly 60\% positive over all aspects. Atmosphere was 80-90\% positive and service was only 60\% positive. \\
\begin{figure*}[!t]
\centering
\subfloat{\includegraphics[width=4.5in]{fig10.pdf}
\label{fig_second_case}}
\caption{Sentiment count for all keywords}
\label{fig_sim}
\end{figure*}
\begin{figure*}[!t]
\centering
\subfloat{\includegraphics[width=3.5in]{fig11.pdf}
\label{fig_second_case}}
\hfil
\subfloat{\includegraphics[width=3.5in]{fig12.pdf}
\label{fig_second_case}}
\caption{Sentiment counts for service and atmosphere}
\label{fig_sim}
\end{figure*}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.

% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].

% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}

% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\section{Conclusion}
In this project, we used SQL queries and NLP to analyze businesses in the Yelp Academic Dataset. With BigQuery, we generated numerous findings by analyzing businesses based on review counts and season, the impact of COVID-19, user behavior, and the relationship between users and reviews. Spark NLP enabled us to calculate the overall annotation sentiment ratio and the individual sentiment ratios for review keywords such as atmosphere and service for selected restaurants. \\
\indent Based on some insights from our analysis, we have a few recommendations for users and businesses. We suggest that Yelp users write reviews in the evening if they want to attract more followers and influence businesses. Because there are more active users during the summertime, restaurants should diversify their menus by adding limited-time special products. While there will be more competition, it is also an opportunity to attract customers and consequently increase revenue. Businesses that wish to understand how users feel about certain aspects such as service quality or pricing and improve them should analyze text reviews. Finally, restaurants should consider branching out to other social media platforms since Yelp is declining. \\
\indent Our future work aims at performing deeper sentiment analysis and opinion mining using Microsoft Azure Text Analytics software. Dependency parsing is a technique that can be used to understand the relationship between words in a sentence; for example, it can determine how adjectives are used to describe the object of sentences. Dependency parsing is useful for analyzing the contents of a review at the sentence level, making it finer than summarizations. Nowadays, numerical data can only provide information about historical trends; text analysis should be a priority for businesses to make better decisions. \\

\section{Lessons Learned}
Processing data, whether through Pandas or ETL tools such as Data Fusion, is necessary to validate the data so that there are no problems when performing queries in a data warehouse. Exploring the data and developing a data model is necessary to understand how the data should be cleaned and stored. \\
\indent When designing a dimensional model for a data warehouse, it is important to identify what attributes belong together in dimension tables. All primary keys in dimension tables become foreign keys in the fact tables; in practice, these keys should be surrogate keys. A fact table should have a date dimension containing at least the day, month, and year of a record. Additional attributes such as holiday in the date dimension allow the analyst to discover interesting trends. \\
\indent Spark NLP provides many pre-trained pipelines to perform tasks such as text classification, sentiment analysis, and named entity recognition. Pre-trained pipelines are convenient for users who need to process data as soon as possible and do not want to spend extra time training custom models. Deploying Spark NLP on clusters will greatly lower the time to insight. \\
\indent Google BigQuery is a serverless data warehouse that executes SQL queries rapidly and provides interactive analysis of massive datasets through Data Studio. It is easy to import data, save personal queries and project queries between multiple users, and has extensive documentation on its SQL syntax. Data Studio has an interface similar to Tableau, which makes it simple for users to drag and drop attributes and measures to create visualizations for reports. \\
\indent Selecting a primary key is important for read performance. Keys need to have the same data type, otherwise it is not possible to join data from other tables or even other datasets. Consequently, the simplicity and utility of surrogate keys can solve the issues arising from the use of natural keys. \\
\indent Dealing with null values is tricky. For text, the rows can be ignored or dropped. For numeric values, SQL functions will return null if null data are used in a calculation. Null data can be handled using functions such as ifnull or imputing values. However, the user should be cautious when replacing these values; interpolating the missing values with the mean or median is better than using zero, which may be meaningless for particular datasets.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

\appendix
\textit{Presentation Skills:} The presentation was rehearsed beforehand so that each section is explained clearly and adheres to the overall time constraint. The powerpoint slides provide basic information with which the speaker adds onto for greater understanding. \\
\textit{Significance to the Real World:} The Yelp dataset contains data about businesses, users, and reviews in a semi-structured format. Each review contains text, which can be mined using sentiment analysis and entity detection to augment the existing metadata. Similar situations arise with social media platforms such as Twitter, in which numerical information is limited, but textual information is plenty. It is necessary to consider additional methods of extracting information from semi-structured data to gain insight into finer details. \\
\textit{Code Walkthrough:} Code for connecting to MySQL and running queries, building a Spark NLP pipeline, and building an ETL pipeline are explained in the presentation. \\
\textit{Report: }Our report provides insight into the project motivation through the introduction and literature review. It describes the data modeling process and methodology and tools used to analyze the dataset. In addition to written results and visualizations, it includes takeaways from the project’s process. \\
\textit{Version Control:} \href{https://github.com/groth00/Data225}{GITHUB LINK} \\
\textit{Discussion / Q\&A:} Our presentation provides the audience with a quick overview of the critical contents of our project in the allotted time and answers in-depth questions in the Q\&A session. \\
\textit{Lessons Learned:} Our lessons learned reflect what we learned in class and extend our knowledge through use of external tools and Cloud services. \\
\textit{Innovation:} We have leveraged BigQuery with SQL to detect historical trends and visualize them using Google Data Studio. Spark NLP was used to enrich the existing data by analyzing the keywords in text reviews and associating them with user sentiments for more in-depth findings. \\
\textit{Teamwork:} The group meets every week to discuss progress on the project and what to do before the next meeting. Each person contributed to the report by writing their own queries and creating visualizations. \href{https://docs.google.com/document/u/1/d/1hZlhFjr7_z8AbwafHHmYNUxNNnhF1n_-k4luKLoJTvQ/}{LINK} \\
\textit{Technical Difficulty:} The difficulty in our project comes from understanding how Spark NLP works and setting up the processing pipeline to annotate text reviews. In addition, there is complexity in setting up ETL pipelines in Google Data Fusion.   \\
\textit{Pair Programming:} We wrote SQL queries in pairs and performed code reviews. See \href{https://docs.google.com/document/u/1/d/1hZlhFjr7_z8AbwafHHmYNUxNNnhF1n_-k4luKLoJTvQ/}{LINK}. \\
\textit{Agile/Scrum:} \href{https://docs.google.com/document/u/1/d/1hZlhFjr7_z8AbwafHHmYNUxNNnhF1n_-k4luKLoJTvQ/}{LINK} \\
\textit{Grammarly:} Grammarly was used to check the final report. \\
\textit{Elevator Pitch Video:} \href{https://www.youtube.com/watch?v=sPYgaDde_nw}{LINK} \\
\textit{Slides:} Our slides introduce the audience to the background of our project, describe the data modeling process, explain our methodology, and provide visualizations for our analysis.  \\
\textit{Demo:} Execution of our saved queries in BigQuery will be demonstrated, and Data Studio will be used to visualize the results.
\textit{Used Unique Tools:} Latex was used to generate the final report.  \\
\textit{Substantial Analysis Using Database Techniques:} BigQuery was used to perform SQL queries on the Yelp dataset and the results were fed into Data Studio to create visualizations.  \\
\textit{New Database / DW Tool:} Google Cloud Data Fusion is an ETL service that allows developers to easily create ETL pipelines using point and click operations to extract data from Cloud Storage, transform it, and sink it into BigQuery. BigQuery is a serverless Data Warehouse that is scalable and can quickly and easily process petabytes of data. \\
\textit{Appropriate Data Models:} A relational model was developed for the business metadata imported into MySQL. The model was normalized into 3NF by separating business categories from business (1:M relationship). A snowflake schema was developed for the data warehouse. It involves the fact table reviews and dimension tables business and user. The dimension table business was normalized to obtain the categories and annotation tables. \\
\textit{ETL:} Google Cloud Data Fusion was used to extract data uploaded into a Cloud Storage bucket, transform each CSV file using an ETL pipeline, and load it into BigQuery tables. As an alternative, AWS Glue was used to crawl the CSV files in an S3 Bucket, transform them by converting data types, and load the result into Redshift.  \\
\textit{Data Cleansing:} Using Pandas, rows with missing values were dropped, irrelevant attributes were dropped, and a column with type array was flattened. Google Cloud Data Fusion was used to drop rows with invalid data types and rename columns before loading the data into BigQuery for analysis. As an alternative, AWS Glue was to convert data types. \\
\textit{Demonstrate How Analytics Supports Business Decisions:} Data visualization can be used to analyze historical trends of businesses, users, and reviews. Businesses can compare themselves based on review count and star rating and see if their performance is decreasing or increasing compared to prior years. The results from Spark NLP can aid businesses by determining which aspects customers feel negative about so that they can take proper measures to improve them. \\
\textit{Used NoSQL:} A NoSQL database was not used since it did not fit the needs of our project. \\
\textit{Used RDBMS:} MySQL and MySQL Workbench were used to load and analyze business metadata. \\
\textit{Used Data Warehouse:} Data was fed from Cloud Storage to BigQuery using an ETL pipeline designed in Data Fusion; BigQuery was then used to query the data using SQL. \\
\textit{DB Connectivity/API Calls:} MySQL connector was used with Python to create tables, insert values, and execute queries.


% use section* for acknowledgment
%\ifCLASSOPTIONcompsoc
%  \section*{Acknowledgments}
%\else
  % regular IEEE prefers the singular form
%  \section*{Acknowledgment}
%\fi


%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{Sentiment Analysis}
M. R. Ching and R. de Dios Bulos, “Improving Restaurants' Business Performance Using Yelp Data Sets through Sentiment Analysis,” Proceedings of the 2019 3rd International Conference on E-commerce, E-Business and E-Government - ICEEG 2019, 2019. DOI:
\url{https://dl-acm-org.libaccess.sjlibrary.org/doi/10.1145/3340017.3340018}
\bibitem{OLAP Dimensions}
Mansmann, Svetlana, et al. “Discovering OLAP Dimensions in Semi-Structured Data.” Information Systems, vol. 44, 18 Oct. 2013, pp. 120–133., doi:\url{https://doi.org/10.1016/ j.is. 2013.09.002}
\bibitem{Google Cloud}
Google Cloud, Cloud Data Fusion \url{<https://cloud.google.com/data-fusion>} (last checked 04.29.2021).
\bibitem{Spark NLP}
John Snow Labs, Spark NLP \url{<https://nlp.johnsnowlabs.com/>} (last checked 04.29.2021).
\bibitem{Data Studio}
Google Cloud, Data Studio \url{<https://datastudio.google.com/navigation/reporting>} (last checked 05.5.2021).
\bibitem{Yelp Decline}
W. Filloon, “Yelp's Heyday Is Over,” Eater, 16-Nov-2018. [Online]. Available: \url{https://www.eater.com/2018/11/16/18094979/yelp-stock-plunge-future-viability-competition-google-instagram-twitter} (last checked 05.03.2021). 
\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


